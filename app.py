# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nZaT4Ib4GqdXVY7QOBOh6HRFsDZQMATF
"""

!pip install torch==2.0.1 transformers==4.32.0 requests numpy scikit-learn nltk schedule streamlit tabulate

import streamlit as st
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch
import requests
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
import nltk
from difflib import SequenceMatcher
import schedule
import time

# Download NLTK stopwords
nltk.download('stopwords')
from nltk.corpus import stopwords

# Preprocessing function
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    stop_words = set(stopwords.words('english'))
    return ' '.join([word for word in text.split() if word not in stop_words])

# Google Scholar search function using SerpAPI
def search_google_scholar(query, api_key, num_results=20):
    url = "https://serpapi.com/search.json"
    params = {
        "engine": "google_scholar",
        "q": query,
        "num": num_results,
        "api_key": api_key
    }
    response = requests.get(url, params=params)
    if response.status_code == 200:
        return response.json()
    else:
        return f"Error: {response.status_code} - {response.text}"

# Prepare the text data from search results
def prepare_text_data(results):
    texts = []
    meta_data = []
    for result in results.get("organic_results", []):
        title = result.get("title", "")
        link = result.get("link", "")
        snippet = result.get("snippet", "")
        abstract = result.get("snippet", "")
        full_text = title + " " + snippet
        cleaned_text = preprocess_text(full_text)

        # Assume the result contains publication_year, journal_impact_factor, author_reputation
        publication_year = int(result.get("publication_year", 2022))
        journal_impact_factor = float(result.get("journal_impact_factor", 0.0))
        author_reputation = float(result.get("author_reputation", 0.0))

        texts.append(cleaned_text)
        meta_data.append({
            "link": link,
            "abstract": abstract,
            'link': result.get('link', 'Link not available'),
            'publication_year': publication_year,
            'journal_impact_factor': journal_impact_factor,
            'author_reputation': author_reputation
        })
    return texts, meta_data # Indentation fixed here

# Automatic Data Refresh system
def refresh_data():
    print("Refreshing data...")
    global texts
    texts = []
    for query in queries:
        results = search_google_scholar(query, api_key, num_results=20)
        if isinstance(results, dict):
            texts += prepare_text_data(results)
    print("Data refresh complete.")

schedule.every().day.at("02:00").do(refresh_data)

# Collecting Data Using the API
api_key = "e73437c2007190db079d36557402977fc4e68a641bcc4fb9f5e43df14f18c950"  # Replace with your SerpAPI key
queries = ["machine learning in healthcare", "artificial intelligence in medicine", "deep learning in medical imaging"]
texts = []
meta_data = []

# Define paper_titles here, if needed
paper_titles = ["Title 1", "Title 2", "Title 3"]  # Replace with actual paper titles

# Comment out the loop iterating over paper_titles if it's not needed
# for title in paper_titles:
#     results = search_google_scholar(title, api_key)

for query in queries:
    results = search_google_scholar(query, api_key, num_results=20)
    if isinstance(results, dict):
        t, m = prepare_text_data(results)
        texts += t
        meta_data += m



# Tokenizing the data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

input_ids = []
attention_masks = []
for text in texts:
    encoded_dict = tokenizer(
        text,
        add_special_tokens=True,
        max_length=64,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

# Converting the lists to tensors
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor([0] * len(input_ids))  # Dummy labels for unsupervised learning

# Creating a DataLoader and a dataloader
from torch.utils.data import DataLoader, TensorDataset

train_dataset = TensorDataset(input_ids, attention_masks, labels)
train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)

# Fine-Tune BERT with Batch Size and Epochs
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=4,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Training the loop
model.train()
for epoch in range(training_args.num_train_epochs):
    for batch in train_dataloader:
        optimizer.zero_grad()
        input_ids, attention_mask, labels = batch  # Unpack the batch
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        print(f"Epoch {epoch}, Loss: {loss.item()}")

# Use the Fine-Tuned Model for Recommendations
def get_bert_embeddings(texts, model, tokenizer):
    model.eval()  # Set model to evaluation mode
    embeddings = []
    with torch.no_grad():  # Disable gradient computation for efficiency
        for text in texts:
            inputs = tokenizer([text], return_tensors='pt', truncation=True, padding=True)
            outputs = model.bert(**inputs)  # Access the BERT part of the model
            embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()
            embeddings.append(embedding)
    return np.vstack(embeddings)

# Example Usage
query = "machine learning in construction"
user_profile_embedding = get_bert_embeddings([query], model, tokenizer)

recommendations = search_google_scholar(query, api_key, num_results=20)
# Assuming prepare_text_data returns a tuple where the first element is the list of texts
texts, _ = prepare_text_data(recommendations) # Unpack the tuple, ignoring the second element
paper_embeddings = get_bert_embeddings(texts, model, tokenizer)
similarities = cosine_similarity(paper_embeddings, user_profile_embedding.reshape(1, -1))
ranked_indices = np.argsort(similarities[:, 0])[::-1]
recommended_papers = [(recommendations['organic_results'][i]['title'], recommendations['organic_results'][i]['link'], similarities[i][0]) for i in ranked_indices[:5]]

# Display the recommendations
for title, link, score in recommended_papers:
    print(f"Title: {title}")
    print(f"Link: {link}")
    print(f"Similarity Score: {score:.4f}")
    print("-" * 80)

from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity
import torch
import numpy as np
import requests
from tabulate import tabulate

# Initialize BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Function to get BERT embeddings
def get_bert_embeddings(texts, model, tokenizer):
    model.eval()  # Set model to evaluation mode
    embeddings = []
    with torch.no_grad():  # Disable gradient computation for efficiency
        for text in texts:
            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
            outputs = model(**inputs)
            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
            embeddings.append(embedding)
    return np.array(embeddings)

# Function to calculate similarity score
def calculate_similarity(embedding1, embedding2):
    return cosine_similarity([embedding1], [embedding2])[0][0]

# Function to get the exact link of the paper given its title
def get_paper_link_by_title(title):
    url = "https://api.crossref.org/works"
    params = {
        'query.title': title,
        'rows': 1
    }
    response = requests.get(url, params=params)
    if response.status_code == 200:
        data = response.json()
        if data['message']['items']:
            paper = data['message']['items'][0]
            doi = paper.get('DOI', None)
            if doi:
                return f"https://doi.org/{doi}"
            else:
                return "DOI not found for this paper."
        else:
            return "No results found."
    else:
        return f"Error: {response.status_code}"

# Metadata of papers
papers_metadata = [
    {
        "title": "Deep Learning for Medical Imaging",
        "link": "",  # Placeholder for actual link
        "publication_year": 2021,
        "journal_impact_factor": 5.5,
        "author_reputation": 0.9,
        "abstract": "This paper discusses the applications of deep learning in medical imaging, covering various techniques and their impact on medical diagnostics.",
        "keywords": ["deep learning", "medical imaging", "healthcare"]
    },
    {
        "title": "Machine Learning in Healthcare",
        "link": "",  # Placeholder for actual link
        "publication_year": 2020,
        "journal_impact_factor": 4.7,
        "author_reputation": 0.8,
        "abstract": "This paper explores the role of machine learning in healthcare, focusing on predictive models and their applications in patient care.",
        "keywords": ["machine learning", "healthcare", "predictive models"]
    }
]

# Fetch the exact link for each paper
for paper in papers_metadata:
    paper['link'] = get_paper_link_by_title(paper['title'])

# Define query text for similarity comparison
query_text = "applications of deep learning in healthcare"

# Get embeddings for query and papers
query_embedding = get_bert_embeddings([query_text], model, tokenizer)[0]
paper_embeddings = get_bert_embeddings([paper['abstract'] for paper in papers_metadata], model, tokenizer)

# Add similarity scores to each paper
for i, paper in enumerate(papers_metadata):
    paper['similarity'] = calculate_similarity(query_embedding, paper_embeddings[i])

# Define filtering and sorting criteria
min_year = 2018
max_year = 2023
min_impact_factor = 2.0
min_author_reputation = 0.7
sort_by = "Relevance"  # Options: Relevance, Recency, Journal Impact Factor, Author Reputation

# Filter and sort the recommendations
filtered_recommendations = []
for paper in papers_metadata:
    if (min_year <= paper['publication_year'] <= max_year and
        paper['journal_impact_factor'] >= min_impact_factor and
        paper['author_reputation'] >= min_author_reputation):
        filtered_recommendations.append(paper)

# Sort based on the selected criteria
if sort_by == "Relevance":
    filtered_recommendations.sort(key=lambda x: x['similarity'], reverse=True)
elif sort_by == "Recency":
    filtered_recommendations.sort(key=lambda x: x['publication_year'], reverse=True)
elif sort_by == "Journal Impact Factor":
    filtered_recommendations.sort(key=lambda x: x['journal_impact_factor'], reverse=True)
elif sort_by == "Author Reputation":
    filtered_recommendations.sort(key=lambda x: x['author_reputation'], reverse=True)

# Display the filtered and sorted recommendations
for paper in filtered_recommendations[:5]:  # Show top 5 recommendations
    print(f"Title: {paper['title']}")
    print(f"Link: {paper['link']}")
    print(f"Publication Year: {paper['publication_year']}")
    print(f"Journal Impact Factor: {paper['journal_impact_factor']}")
    print(f"Author Reputation: {paper['author_reputation']}")
    print(f"Abstract: {paper['abstract']}")
    print(f"Similarity: {paper['similarity']:.4f}")
    print("-" * 80)

# Prepare data for tabular display
table = []
for paper in filtered_recommendations[:5]:  # Show top 5 recommendations
    table.append([paper['title'], paper['link'], paper['publication_year'], paper['journal_impact_factor'], paper['author_reputation'], paper['abstract'], f"{paper['similarity']:.4f}"])

# Display the table
headers = ["Title", "Link", "Publication Year", "Journal Impact Factor", "Author Reputation", "Abstract", "Similarity"]
print(tabulate(table, headers=headers, tablefmt="grid"))

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
from difflib import SequenceMatcher

# Example of metadata with combined relevance score
papers_metadata = [
    {
        "title": "Deep Learning for Medical Imaging",
        "link": "https://example.com/paper1",
        "publication_year": 2021,
        "journal_impact_factor": 5.5,
        "author_reputation": 0.9,
        "similarity": 0.95,
        "citations": 150
    },
    {
        "title": "Machine Learning in Healthcare",
        "link": "https://example.com/paper2",
        "publication_year": 2020,
        "journal_impact_factor": 4.7,
        "author_reputation": 0.8,
        "similarity": 0.89,
        "citations": 200
    },
    {
        "title": "AI in Medical Diagnostics",
        "link": "https://example.com/paper3",
        "publication_year": 2019,
        "journal_impact_factor": 6.2,
        "author_reputation": 0.85,
        "similarity": 0.88,
        "citations": 180
    },
    {
        "title": "Neural Networks in Medicine",
        "link": "https://example.com/paper4",
        "publication_year": 2021,
        "journal_impact_factor": 5.1,
        "author_reputation": 0.9,
        "similarity": 0.86,
        "citations": 140
    },
    # Add more paper metadata as needed
]

# Combine relevance score
for paper in papers_metadata:
    paper['combined_relevance'] = (
        0.5 * paper['similarity'] +
        0.3 * (paper['citations'] / 1000) +  # Normalizing citations
        0.2 * paper['journal_impact_factor']
    )

# Sort by combined relevance
sorted_papers = sorted(papers_metadata, key=lambda x: x['combined_relevance'], reverse=True)

def average_precision_at_k(actual, predicted, k):
    if not actual:
        return 0.0

    if len(predicted) > k:
        predicted = predicted[:k]

    score = 0.0
    num_hits = 0.0

    for i, p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:  # only count hits once
            num_hits += 1.0
            score += num_hits / (i + 1.0)

    return score / min(len(actual), k)

# Example of actual relevant papers
actual_relevant_papers = [
    ["deep learning for medical imaging", "machine learning in healthcare"]
]

# Normalization and partial match functions
def normalize_title(title):
    title = title.lower()
    title = re.sub(r'[^a-zA-Z0-9\s]', '', title)
    return title.strip()

def partial_match_score(a, b):
    return SequenceMatcher(None, a, b).ratio()

# Function to calculate MAP and other metrics
def calculate_metrics(actual_relevant_papers, predicted_papers, k):
    precision = precision_at_k_partial(actual_relevant_papers[0], predicted_papers, k)
    recall = recall_at_k(actual_relevant_papers[0], predicted_papers, k)
    f1 = f1_score(precision, recall)
    map_score = mean_average_precision_at_k(actual_relevant_papers, [predicted_papers], k)
    return precision, recall, f1, map_score

# Set the number of papers you want to include
k = 5  # Increase this to get more predicted papers
predicted_papers = [normalize_title(paper['title']) for paper in sorted_papers[:k]]

actual_relevant_papers_normalized = [[normalize_title(title) for title in actual_list] for actual_list in actual_relevant_papers]

print("Actual Relevant Papers:")
for paper in actual_relevant_papers_normalized[0]:
    print(f"- {paper}")

print("\nPredicted Papers:")
for paper in predicted_papers:
    print(f"- {paper}")

# Calculate metrics
precision, recall, f1, map_score = calculate_metrics(actual_relevant_papers_normalized, predicted_papers, k)

# Display the results
print(f"Precision@{k}: {precision:.4f}")
print(f"Recall@{k}: {recall:.4f}")
print(f"F1 Score@{k}: {f1:.4f}")
print(f"MAP@{k}: {map_score:.4f}")

# Allow Users to Provide Feedback
def get_feedback(recommended_papers):
    feedback = []
    print("\nPlease rate the relevance of each recommended paper on a scale from 1 to 5 (1 = Not relevant, 5 = Very relevant):\n")
    for i, (title, link, score) in enumerate(recommended_papers, start=1):
        rating = int(input(f"Relevance of paper {i} (Title: {title}): "))
        feedback.append((title, link, score, rating))
    return feedback

# Step 11: Store or Use Feedback for Further Analysis or Model Improvement
def analyze_feedback(feedback):
    avg_rating = np.mean([rating for _, _, _, rating in feedback])
    print(f"\nAverage feedback rating: {avg_rating:.2f}")
    if avg_rating < 3:
        print("Feedback indicates that the recommendations may need improvement.")
    else:
        print("Feedback indicates that the recommendations are generally relevant.")


        # Placeholder function - replace with your actual logic
def get_user_query_and_recommend():
    """
    This function should handle:
    1. Getting the user's query
    2. Generating recommendations based on the query
    3. Returning the list of recommended papers
    """
    # Replace this with your actual implementation
    recommended_papers = [
        ("Sample Paper 1", "https://example.com/paper1", 0.9),
        ("Sample Paper 2", "https://example.com/paper2", 0.8),
        # ... add more recommendations here
    ]
    return recommended_papers

# Main loop for user interaction
def main():
    continue_interaction = True
    while continue_interaction:
        recommended_papers = get_user_query_and_recommend()
        feedback = get_feedback(recommended_papers)
        analyze_feedback(feedback)

        another_query = input("\nWould you like to enter another query? (yes/no): ").strip().lower()
        if another_query != 'yes':
            continue_interaction = False

# Run the interaction loop
if __name__ == "__main__":
    main()

def hybrid_recommendation_system(user_query, user_profile_embedding, user_history, all_users_history, k=5):
    # Content-based recommendations using BERT
    recommendations = search_google_scholar(user_query, api_key, num_results=20)
    texts = prepare_text_data(recommendations)
    # Ensure that 'texts' is a list of strings, each representing a document
    if not isinstance(texts, list) or not all(isinstance(text, str) for text in texts):
        texts = [str(text) for text in texts]  # Convert each element to a string
    paper_embeddings = get_bert_embeddings(texts, model, tokenizer)
    similarities = cosine_similarity(paper_embeddings, user_profile_embedding.reshape(1, -1))
    content_based_papers = np.argsort(similarities[:, 0])[::-1][:k]

    # Collaborative filtering recommendations
    collaborative_papers = collaborative_filtering(user_history, all_users_history, k)

    # Combining both recommendations
    final_recommendations = list(set(content_based_papers) | set(collaborative_papers))

    # Fetching and display final recommendations
    final_papers = [(recommendations['organic_results'][i]['title'], recommendations['organic_results'][i]['link']) for i in final_recommendations[:k]]
    for title, link in final_papers:
        print(f"Title: {title}")
        print(f"Link: {link}")
        print("-" * 80)

# These lines should be outside the function, at the same indentation level
user_query = "neural network"
user_profile_embedding = get_bert_embeddings([user_query], model, tokenizer)
user_history = np.array([[1, 0, 0], [0, 1, 0]])
all_users_history = np.array([[1, 0, 1], [0, 1, 0]])
hybrid_recommendation_system(user_query, user_profile_embedding, user_history, all_users_history)

